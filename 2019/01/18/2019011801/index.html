<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="reinforcement learning,">





  <link rel="alternate" href="/atom.xml" title="张萌的blog" type="application/atom+xml">






<meta name="description" content="[TOC] 1为什么会出现分层强化学习​        强化学习要研究的问题是智能体（agents）如何在一个环境（environment）中学到一定的策略（policy），使得长期的奖赏（reward）最大。 但是传统的强化学习方法面临着维度灾难，即当环境较为复杂或者任务较为困难时，agent的状态（state）空间过 大，会导致需要学习的参数以及所需的存储空间急速增长，强化学习难以取得理想的效">
<meta name="keywords" content="reinforcement learning">
<meta property="og:type" content="article">
<meta property="og:title" content="分层强化学习">
<meta property="og:url" content="http://yoursite.com/2019/01/18/2019011801/index.html">
<meta property="og:site_name" content="张萌的blog">
<meta property="og:description" content="[TOC] 1为什么会出现分层强化学习​        强化学习要研究的问题是智能体（agents）如何在一个环境（environment）中学到一定的策略（policy），使得长期的奖赏（reward）最大。 但是传统的强化学习方法面临着维度灾难，即当环境较为复杂或者任务较为困难时，agent的状态（state）空间过 大，会导致需要学习的参数以及所需的存储空间急速增长，强化学习难以取得理想的效">
<meta property="og:locale" content="zh">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/131433102201.jpg">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/131433102201.jpg">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/02.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/03.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/04.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/05.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/06.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/07.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/08.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/09.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/10.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/11.png">
<meta property="og:image" content="http://yoursite.com/2019/01/18/2019011801/2019011801/12.png">
<meta property="og:updated_time" content="2019-01-20T15:50:36.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="分层强化学习">
<meta name="twitter:description" content="[TOC] 1为什么会出现分层强化学习​        强化学习要研究的问题是智能体（agents）如何在一个环境（environment）中学到一定的策略（policy），使得长期的奖赏（reward）最大。 但是传统的强化学习方法面临着维度灾难，即当环境较为复杂或者任务较为困难时，agent的状态（state）空间过 大，会导致需要学习的参数以及所需的存储空间急速增长，强化学习难以取得理想的效">
<meta name="twitter:image" content="http://yoursite.com/2019/01/18/2019011801/131433102201.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/01/18/2019011801/">





  <title>分层强化学习 | 张萌的blog</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">张萌的blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">还行</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/18/2019011801/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhang Meng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张萌的blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">分层强化学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-18T20:48:07+08:00">
                2019-01-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/reinforcement-learning/" itemprop="url" rel="index">
                    <span itemprop="name">reinforcement learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h1><span id="1为什么会出现分层强化学习">1为什么会出现分层强化学习</span></h1><p>​        强化学习要研究的问题是智能体（agents）如何在一个环境（environment）中学到一定的策略（policy），使得长期的奖赏（reward）最大。 但是传统的强化学习方法面临着维度灾难，即当环境较为复杂或者任务较为困难时，agent的状态（state）空间过 大，会导致需要学习的参数以及所需的存储空间急速增长，强化学习难以取得理想的效果。 为了解决维度灾难，研究者提出了分层强化学习（hierarchical reinforcement learning，HRL）。HRL 的主要目标是将复杂的问题分解成多个小问题，分别解决小问题从而达到解决原问题的目的。 近些年来，人们认为分层强化学习基本可以解决强化学习的维度灾难问题，转而将研究方向转向如何将复杂的问题抽象成不同的层级，从而更好地解决这些问题。 </p>
<h1><span id="2背景知识">2背景知识</span></h1><p>​        本部分主要介绍强化学习、马尔可夫决策过程mdp和半马尔可夫决策过程semi-mdp的相关内容和背景知识。</p>
<h2><span id="21rl与mdp">2.1RL与MDP</span></h2><p>​        强化学习主要研究的问题是agent通过和environment直接交互的过程来学习策略，使得达到目标后的总体奖励最大。大多数关于强化学习RL的研究都建立在马尔可夫决策过程MDP上。</p>
<h4><span id="211马尔可夫决策过程的定义">2.1.1马尔可夫决策过程的定义</span></h4><p>​        MDP可以表示为一个五元组〈Ｓ，Ａ，Ｐ，Ｒ，γ〉。 其中，Ｓ为环境状态state的有限集合，集合中某个状态表示为s∈S；A为动作 action的有限集合，集合中某个动作表示为 a∈A，A 为状态s下可执行的动作集合；P状态转移方程，P(s′|s,a) 表示在状态s执行动作a后将以 P的概率跳转到状态 s′；R为奖赏函数；γ 为折损系数， ０≤γ≤１。 </p>
<p>​        假设一个agent观察到自己的状态s，此时它选择一个动作a，它会得到一个即时的奖赏ｒ=R(s,a)，然后以 P(s′ |s,a) 的概率达到下一个状态s′。 马尔可夫决策过程有马尔可夫性，即系统的下个状态只与当前状态有关，与之前的状态无关。 当马尔可夫决策过程中作出决策时，只需要考虑当前的状态，而不需要历史数据，这样大大降低了问题的复杂度。 </p>
<img src="/2019/01/18/2019011801/131433102201.jpg" title="This is an example image">
<p><img src="2019011801\131433102201.jpg" alt="01"></p>
<h4><span id="212值函数">2.1.2值函数</span></h4><p>​        强化学习需要agent学习到一个策略 π，通过 π (s,a) 的值来指导agent进行动作的选择。衡量这个策略的标准依赖于回报，增强学习往往具有延迟回报的特点 : 如果在第n步输掉了棋，那么只有状态sn和动作an获得了立即回报r(sn,an)=-1，前面的所有状态立即回报均为0。所以对于之前的任意状态s和动作a，立即回报函数r(s,a)无法说明策略的好坏。因而需要定义值函数 (value function，又叫效用函数) 来表明当前状态下策略π的长期影响。 </p>
<p>​        令Ｖ(ｓ) 表示在状态s的<strong>值函数</strong>，即从状态s开始一直按照某个策略执行最终达到终止状态的期望累计奖赏。我们的目标是希望学到一个最优的策略 π，最大化每一个状态下的Ｖ值。</p>
<p><img src="2019011801\02.png" alt="02"></p>
<p>是值函数最常见的形式，式中γ∈[0,1]称为折合因子，表明了未来的回报相对于当前回报的重要程度。特别的，γ=0时，相当于只考虑立即不考虑长期回报，γ=1时，将长期回报和立即回报看得同等重要。接下来把这个值函数展开，其中ri表示未来第i步回报，s’表示下一步状态，则有：</p>
<p><img src="2019011801\03.png" alt="03"></p>
<p>给定策略π和初始状态s，则动作a=π(s)，下个时刻将以概率p(s’|s,a)转向下个状态s’，那么上式的期望可以拆开，可以重写为：</p>
<p><img src="2019011801\04.png" alt="04"></p>
<p>π和初始状态s是我们给定的，而初始动作a是由策略π和状态s决定的，即a=π(s)。</p>
<h4><span id="213动作值函数">2.1.3动作值函数</span></h4><p>​        除了值函数，期望累计奖赏也可以用<strong>动作-值函数</strong>来表示，令 Ｑ(s,a)  在状态s执行动作a之后按照某个策略执行直到达到终止状态的期望累计奖赏。我们也希望通过学习到一个最优的Ｑ函数，使agent可以直接通过Ｑ函数来选择当前状态下应该执行的动作。定义动作值函数(action value functionQ函数)如下：</p>
<p><img src="2019011801\05.png" alt="05"></p>
<p>给定当前状态s和当前动作a，在未来遵循策略π，那么系统将以概率p(s’|s,a)转向下个状态s’，上式可以重写为：</p>
<p><img src="2019011801\06.png" alt="06"></p>
<p>在Qπ(s,a)中，不仅策略π和初始状态s是我们给定的，当前的动作a也是我们给定的，这是Qπ(s,a)和Vπ(a)的主要区别。</p>
<p>​        知道值函数的概念后，一个MDP的最优策略可以由下式表示：</p>
<p><img src="2019011801\07.png" alt="07"></p>
<p>即我们寻找的是在任意初始条件s下，能够最大化值函数的策略π*。</p>
<p>​        经过多年的研究，已经出现一些算法，致力于解决传统的强化学习问题，比如Ｑ⁃learning、蒙特卡洛方法（Monte-Carlo learning）、时 序 差 分方法（temporal-difference learning）等。其中 Ｑ⁃learning 方法常常在分层强化学习中被使用。Ｑ⁃learning通 过不断迭代更新Ｑ函数的值来逼近最优的Ｑ∗。</p>
<h4><span id="214qlearning">2.1.4Ｑ⁃learning</span></h4><h2><span id="22-smdp">2.2 SMDP</span></h2><p>​        青蛙在荷叶上跳动，在每个不同的荷叶上表示不同的状态，从一个荷叶跳到另一个荷叶表示状态的转移，该转移过程只依赖于现在所处荷叶，而与以前呆过的荷叶无关，如果只考虑青蛙跳跃的时刻序列，这个过程就是离散时间的马氏过程，如果考虑青蛙在荷叶上呆了一段时间，且这段时间是服从指数分布的，那这个过程是连续时间马氏过程，如果时间是非指数分布的，那这个过程是半马氏分布。</p>
<p>​        马尔可夫决策过程中，选择一个动作后，agent 会立刻根据状态转移方程P跳转到下一个状态，而在半马尔可夫决策过程SMDP中，当前状态到下 一个状态的步数是个随机变量 τ，即在某个状态s下选择一个动作a，经过 τ 步才会以一个概率转移到下一个状态s′。 此时的状态转移概率是s 和 τ 的联合概率P（s′,τ |s,a）。 根据 τ 的定义域不同，SMDP所定义的系统也有所不同。 当 τ 的取值为实数值，则SMDP构建了一个连续时间-离散事件系统；而当 τ 的取值为正整 数，则是一个离散时间SMDP。 出于简单考虑，绝大部分分层强化学习都是在离散时间SMDP上进行讨论。 </p>
<h1><span id="3分层强化学习">3分层强化学习</span></h1><p>​        分层强化学习是将复杂的强化学习问题分解成一些容易解决的子问题，通过分别解决这些子问题，最终解决原本的强化学习问题。 常见的分层强化学习方法可以大致分为四种，分别为基于选项的强化学习、基于分层抽象机 的分层强化学习、基于 MaxQ函数分解的分层强化学习，以及端到端的分层强化学习。本节将对它们逐一进行探讨。 </p>
<p>​        对于HRL，抽象是至关重要的。通过抽象可去掉子任务中一些冗余或无关的信息，简化问题的表示，更容易、更方便地解决问题。在HRL中，抽象包括时间抽象 (动作抽象) 和空间抽象 (状态抽象) 。</p>
<p>​        时间抽象是一种将若干动作封装在一起， 被视为单个抽象动作，其主要目的在于充分利用问题的分层结构。每个抽象动作对应一个定义在环境状态子集上、具有终止条件的闭环局部策略，研究者有时称这些策略为抉择(options)、时间扩展的动作(temporally-extended actions)。抽象动作允许 agent 按照其局部策略用多个时间步调用所封装的底层动作，直到终止。【个人理解：其实就是把几个小动作封装成一个整体，在状态s下通过全局策略的决定进入这个大的整体之后，按照这个整体内部自己的局部策略执行小动作，在达到终止条件的时候结束这个整体动作，达到一个宏观上的新状态，再通过全局策略选择下一个整体动作进入。】</p>
<p>​        空间抽象可分为两种情况，一种是将若干状态封装在一起，被视为单个抽象状态；另一种是针对环境状态可用矢量表示，即状态可分解的情形，删掉与子任务无关的状态变量，从而达到简化状态表示的目的。</p>
<h3><span id="31基于选项的分层强化学习">3.1基于选项的分层强化学习</span></h3><p>​        基于option的分层强化学习的过程如下：假设 agent 当前在某个状态，选择一个option，通过这个 option的策略，agent 选择了一个动作或者另一个 option。 若选择了一个动作，则直接执行转移到下一 个状态；若选择了另一个 option，则用选择的新 option继续选择，直到最后得出一个动作。</p>
<p>​        最简单的option为直接定义在MDP上的马尔可夫option，是一个三元组 &lt; I , Π , T&gt; 。其中：l∈S是option入口状态集合； Π：s×A[0，1]为option内部策略；T：S [ 0，1 ]为option终止条件，也就是出口。当option被激活后，agent根据 Π 选择执行的动作，最后根据 T 随机终止。对于任意动作a∈A可看成是单步opfion。</p>
<p>​        另一种比较灵活的option是半马尔可夫option，其内部策略和终止条件均依赖于option激活后所经历的状态、动作和回报等历史。令Ω表示历史集，形式上半马尔可夫option也是一 个三元组&lt; I , Π , T&gt;。I 的含义与马尔可夫option的 I 相同，但内部策略和终止条件分别定义为 Π ：ΩXA→[0，1]和T：Ω→ [0，1]。 </p>
<p>​         令O为option集合。如果将策略定义在options之上，即定义为 μ ：s×O→[0，1]，则形成options分层机制。在这种机制下，agent根据 μ 选择一个option执行，直到终止，agent再依照 μ 选择另一个option执行，依此类推。如果将O叠加到 MDP上，将产生一个离散时间SMDP，因而可使用SMDP所有的理论和算法。</p>
<p>​        把MDP单步模型里的action替换成options并去掉折合因子，就可以得到最优值函数和最优option值函数：</p>
<p><img src="2019011801\08.png" alt="08"></p>
<p>​        此时的Q-Learning的更新公式为：</p>
<p><img src="2019011801\09.png" alt="09"></p>
<p>其中，ak为第k轮迭代时的学习率，τ 表示option  o 在执行 τ 步之后在状态 St+τ 停止，而 ｏ′ 为在 ｏ 执行 结束后的下一个option。 可以注意到，<strong>当所有的option均为one⁃step option时，这个Ｑ⁃learning就退 化为普通的Ｑ⁃learning过程。</strong> </p>
<h3><span id="32基于分层抽象机的分层强化学习">3.2基于分层抽象机的分层强化学习</span></h3><p>​        分层抽象机（hierarchies of abstract machines,HAMs）是Parr和Russell提出的方法。和option的方法类似，HAMs的方法也是建立在SMDP 的理论基础之上的。 HAMs的主要思想是将当前所在状态以及有限状态机的状态结合考虑，从而选择不同的策略。 </p>
<p>​        令M为一个有限MDP，{Hi}为一个有限状态机集合，每个有限状态机都有一个状态集合Si、一个概率转移方程以及一个随机函数fi。状态集合包括：action、call、choice、stop。</p>
<ul>
<li><p>action类型的状态会根据状态机的具体状态执行一个MDP中的动作。</p>
</li>
<li><p>call类型的状态时，当前状态机被挂起，根据该状态机的当前状态得到下一个状态机，对其用随机函数初始化（得到状态）。</p>
</li>
<li><p>choice类型的状态时，不确定地选择当前状态机地下一个状态。</p>
</li>
<li><p>stop类型的状态时，停止当前状态机的活动，恢复调用它的状态机的活动，同时agent根据之前的动作进行动作转移并得到奖励。（如果某个状态机H刚被调用就被随机函数初始化到了stop状态，返回的时候没有选出来要执行的动作，则M保持当前的状态。）</p>
<p><img src="2019011801\10.png" alt="10"></p>
</li>
</ul>
<p>  ​         总之，options和HAM都是把MDP转换为了单个SMDP问题，而MAXQ和HEXQ方法创建了多个SMDP的分层，使得每个SMDP同时学习。</p>
<h3><span id="33基于-maxq函数分解的分层强化学习">3.3基于 MaxQ函数分解的分层强化学习</span></h3><p>  ​        MaxQ 值分解是由Dietterich提出的另外一种分层强化学习的方法。首先把一个马尔可夫决策过程M分解成多个子任务{M0，M1,…,Mn}, M0为根子任务，解决了它就意味着解决了原问题M。每一个子任务都有一个终止断言T和一个动作集合A。A中的元素可以是其他的子任务也可以是一个MDP中的action。子任务的目标是转移到一个状态可以满足终止断言使得子任务完成并终止。</p>
<p>  图１为利用MaxQ方法解决出租车问题的任务划分示意图：</p>
<p>  <img src="2019011801\11.png" alt="11"></p>
<p>  ​        出租车问题是指一个出租车agent需要到特定位置接一位乘客并且把他送到特定的位置让其下车。 一共有6个动作，分别是上车（pickup）、下车 (dropoff），以及向东南西北四个方向开车的动作。 这里使用MaxQ方法，将原问题分解成了get和put两个子任务，这两个子任务又进行分解，get分解成一个基本动作pickup和一个子任务navigate，而put也分解成了一个基本动作dropoff和一个子任务navigate。 子任务navigate(t)表示t时刻应该开车的方向。 对于这个强化学习问题，agent首先选择get， 然后get子问题navigate，直到到达乘客所在地，然后get选择pickup动作，乘客上车。 之后agent选择put子任务，put子任务选择navigate，直到到达乘客目的地，之后put子任务选择dropoff动作，乘客下车，任务完成。 </p>
<h3><span id="34端到端的分层强化学习">3.4端到端的分层强化学习</span></h3><p>  ​         上述3种方法，都需要人工来做很多工作， 比如人工进行option的选取，人工进行HAMs的构 建，人工划分子任务等。 人工设计不仅耗时耗力， 并且会直接影响最终强化学习结果的好坏。 近些年来人们关注如何让agent自己学到合理的分层抽象，而非人为进行划分和指定。</p>
<p>  。。。。。。</p>
<h2><span id="4对比">4对比</span></h2><p>  <img src="2019011801\12.png" alt="12"></p>
<p>分层最优指的是：该策略是与分层结构兼容的所有分层策略中具有最高累计回报的策略。</p>
<p>递归最优指的是：一个分层策略使得对于每个子任务，对应的策略都是最优的。</p>
<p>分层最优策略和递归最优策略的根本区别在于，前者的子任务不仅要考虑所调用的下级子任务，也要考虑如何加入到上级子任务中；而后者的子任务没有考虑其应用环境。</p>
<p>安全状态抽象是指值函数在抽象前后保持一致的状态抽象。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/01/17/2019011702/" rel="next" title="Container With Most Water">
                <i class="fa fa-chevron-left"></i> Container With Most Water
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/01/19/20190119/" rel="prev" title="Integer to Roman and roman to integer">
                Integer to Roman and roman to integer <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhang Meng</p>
              <p class="site-description motion-element" itemprop="description">主要内容是Leetcode和个人学习记录</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">1为什么会出现分层强化学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">2背景知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">2.1.</span> <span class="nav-text">2.1RL与MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">2.1.0.1.</span> <span class="nav-text">2.1.1马尔可夫决策过程的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">2.1.0.2.</span> <span class="nav-text">2.1.2值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">2.1.0.3.</span> <span class="nav-text">2.1.3动作值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">2.1.0.4.</span> <span class="nav-text">2.1.4Ｑ⁃learning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 SMDP</span></a></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">3.</span> <span class="nav-text">3分层强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-number">3.0.1.</span> <span class="nav-text">3.1基于选项的分层强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-number">3.0.2.</span> <span class="nav-text">3.2基于分层抽象机的分层强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-number">3.0.3.</span> <span class="nav-text">3.3基于 MaxQ函数分解的分层强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-number">3.0.4.</span> <span class="nav-text">3.4端到端的分层强化学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">3.1.</span> <span class="nav-text">4对比</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhang Meng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
